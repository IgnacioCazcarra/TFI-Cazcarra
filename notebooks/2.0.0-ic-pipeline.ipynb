{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7e82d8",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "- Predicciones para cardinalidades\n",
    "- Capa de MySQL code validation?\n",
    "- Como ordeno las tablas en orden? Ejemplo, si una tiene una FK de una tabla que todav√≠a no se cre√≥. Respuesta -> https://dba.stackexchange.com/questions/258248/table-creation-order dejas las FK y las PK para despu√©s de los create table asi no tenes problemas.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a92e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab04a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.training_utils.training import load_model\n",
    "from src.line_detection.hough import *\n",
    "from ast import literal_eval\n",
    "from src.ocr_utils.ocr import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3bd6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "from paddleocr import PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb8b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODELS = \"/home/nacho/TFI-Cazcarra/data/models\"\n",
    "PATH_YOLO = \"/home/nacho/TFI-Cazcarra/yolov3/runs/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f6eb5",
   "metadata": {},
   "source": [
    "## Realizo predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d64ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/nacho/TFI-Cazcarra/data/imagenes_diagramas/ERDiagramsMySQL-5.png'\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "img_tensor = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9870d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(img, xmin, ymin, xmax, ymax, score, label): \n",
    "    txt = get_class_name(label, le_dict) + ' ' + str(score)\n",
    "    img = cv2.putText(img, txt, (int(xmin), int(ymin)),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,255), 1)\n",
    "\n",
    "    return cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), \n",
    "                         (255,0,0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41afe6e0",
   "metadata": {},
   "source": [
    "### 1. Extraigo las tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75bc564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet_tablas = load_model(os.path.join(PATH_MODELS, \"model_best_tablas_retinanet.pt\"))\n",
    "faster_rcnn_tablas = load_model(os.path.join(PATH_MODELS, \"model_best_tablas_fasterrcnn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b4721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/nacho/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 üöÄ 2023-2-8 Python-3.8.10 torch-1.13.0+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 261 layers, 61497430 parameters, 0 gradients, 154.5 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "yolo_tablas = torch.hub.load('ultralytics/yolov5', 'custom', \\\n",
    "                             os.path.join(PATH_YOLO, \"exp5\", \"weights\", \"best.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416a2c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m valid_idx_1 \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mnms(tablas_1[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m], tablas_1[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      4\u001b[0m coords_1 \u001b[38;5;241m=\u001b[39m tablas_1[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m][valid_idx_1]\n\u001b[0;32m----> 5\u001b[0m scores_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtablas_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m][valid_idx_1]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tablas_1 = retinanet_tablas([img_tensor])\n",
    "valid_idx_1 = torchvision.ops.nms(tablas_1[1][0]['boxes'], tablas_1[1][0]['scores'], 0.1)\n",
    "\n",
    "coords_1 = tablas_1[1][0]['boxes'][valid_idx_1]\n",
    "scores_1 = tablas_1[1][1]['scores'][valid_idx_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d02f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablas_2 = faster_rcnn_tablas([img_tensor])\n",
    "valid_idx_2 = torchvision.ops.nms(tablas_2[1][0]['boxes'], tablas_2[1][0]['scores'], 0.1)\n",
    "\n",
    "coords_2 = tablas_2[1][0]['boxes'][valid_idx_1]\n",
    "scores_2 = tablas_2[1][1]['scores'][valid_idx_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cab39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablas_3 = yolo_tablas(Image.open('/home/nacho/TFI-Cazcarra/data/imagenes_diagramas/ERDiagramsMySQL-1.png').convert(\"RGB\"))\n",
    "\n",
    "#x1 y1 x2 y2 score class\n",
    "coords_3 = tablas_3.xyxyn[0][:, :4]\n",
    "scores_3 = tablas_3.xyxyn[0][:, 4]\n",
    "\n",
    "valid_idx_3 = torchvision.ops.nms(coords_3, scores_3, 0.1)\n",
    "\n",
    "# Falta filtrar ac√°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3135c5",
   "metadata": {},
   "source": [
    "### 2. Extraigo las cardinalidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d486c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet_tablas = load_model(os.path.join(PATH_MODELS, \"model_best_cardinalidades_retinanet.pt\"))\n",
    "faster_rcnn_tablas = load_model(os.path.join(PATH_MODELS, \"model_best_cardinalidades_fasterrcnn.pt\"))\n",
    "\n",
    "yolo_tablas = torch.hub.load('ultralytics/yolov5', 'custom', \\\n",
    "                             os.path.join(PATH_YOLO, \"exp3\", \"weights\", \"best.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace65500",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Dividir la imagen en frames y etc etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4e86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6c08f62",
   "metadata": {},
   "source": [
    "## Encuentro las l√≠neas y junto con las cardinalidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5f6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSET_TABLAS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfbae858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lines(img_path):\n",
    "    img_basename = os.path.basename(img_path)\n",
    "    tablas, cardinalidades = get_tablas(img_basename)\n",
    "    #Agrandamos un toque las cajas\n",
    "    offset = np.array([-OFFSET_TABLAS, -OFFSET_TABLAS, OFFSET_TABLAS, OFFSET_TABLAS]).reshape(1,4)\n",
    "    tablas = np.sum([tablas, offset])\n",
    "\n",
    "    #No le paso las cardinalidades.\n",
    "    img, all_lines = apply_hough(img_path, tablas, [])\n",
    "\n",
    "    all_points = lines_to_points(all_lines)\n",
    "    lines = hough_detecting(all_points)\n",
    "\n",
    "    return unify_cardinalidades(img, lines, cardinalidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf5ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_points(x, y, max_dst_per_points=2):\n",
    "    dst_line = math.dist(x,y)\n",
    "    parts = dst_line // max_dst_per_points\n",
    "    return np.linspace(x, y, int(parts)+1).tolist()\n",
    "\n",
    "\n",
    "def dist_func(comb):\n",
    "    return math.dist(comb[0], comb[1])\n",
    "\n",
    "\n",
    "def get_centroid(cardinalidad):\n",
    "    center_x  = cardinalidad[0] + (cardinalidad[2] - cardinalidad[0])/2\n",
    "    center_y  = cardinalidad[1] + (cardinalidad[3] - cardinalidad[1])/2\n",
    "    return (center_x, center_y)\n",
    "\n",
    "def nearest_tabla_from_cardinalidad(cardinalidad, tablas):\n",
    "    # Dadas las coords de una cardinalidad, devolver a qu√© tabla est√° m√°s cerca.\n",
    "    # TODO: Puedo mezclarlo con el de OCR y reemplazar las coordenadas por el nombre de la\n",
    "    # tabla... asi se entiende mejor. Esto quiz√°s no hace falta pero no estar√≠a mal.\n",
    "    cardinalidad = literal_eval(cardinalidad)\n",
    "    cardinalidad_centroid = get_centroid(cardinalidad)\n",
    "    dict_dist_tablas = {}\n",
    "    for t in tablas:\n",
    "        tabla_points = []\n",
    "        # De x1y1 a x2y1\n",
    "        tabla_points += line_to_points((t[0], t[1]), (t[2], t[1]))\n",
    "        # De x1y1 a x1y2\n",
    "        tabla_points += line_to_points((t[0], t[1]), (t[0], t[3]))\n",
    "        # De x1y2 a x2y2\n",
    "        tabla_points += line_to_points((t[0], t[3]), (t[2], t[3]))\n",
    "        # De x2y1 a x2y2\n",
    "        tabla_points += line_to_points((t[2], t[1]), (t[2], t[3]))\n",
    "        dist_cardinalidad = [(cardinalidad_centroid, point) for point in tabla_points]\n",
    "        min_combination = min(dist_cardinalidad, key=dist_func)\n",
    "        dict_dist_tablas[\",\".join([str(c) for c in t])] = dist_func(min_combination)\n",
    "    nearest_tabla = min(dict_dist_tablas, key=dict_dist_tablas.get)\n",
    "    return [int(c) for c in nearest_tabla.split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d50bf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "img_basename = os.path.basename(img_path)\n",
    "tablas, _ = get_tablas(img_basename)\n",
    "\n",
    "for line_name, line in find_lines(img_path).items():\n",
    "    cardinalidades = line.split(\"|\")\n",
    "    tabla_a = nearest_tabla_from_cardinalidad(cardinalidades[0], tablas)\n",
    "    tabla_b = nearest_tabla_from_cardinalidad(cardinalidades[1], tablas)\n",
    "    pairs.append((tabla_a, tabla_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae7c211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([289, 16, 440, 150], [18, 89, 211, 344]),\n",
       " ([291, 191, 449, 383], [18, 89, 211, 344]),\n",
       " ([291, 191, 449, 383], [516, 241, 664, 334])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0391a2c7",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99074250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez que tenemos la lista de tuplas con las conexiones de las tablas, podemos sacar\n",
    "# las FK y de ahi, descartar los dem√°s y poder sacar la/s PK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40644237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose algo and language.\n",
    "ocr = PaddleOCR(use_angle_cls=False, lang='en', show_log=False, \n",
    "                det_algorithm_dir=\"/home/nacho/TFI-Cazcarra/ocr/db/\", \n",
    "                rec_algorithm_dir=\"/home/nacho/TFI-Cazcarra/ocr/svtr/\", det_db_score_mode=\"slow\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33199628",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "results = []\n",
    "\n",
    "img_arr = np.array(img)\n",
    "\n",
    "for t in tablas:\n",
    "    tabla_cropped = img_arr[t[1]:t[3], t[0]:t[2]]\n",
    "    tabla_cropped = reescale(tabla_cropped)\n",
    "    if len(tabla_cropped.shape) == 3 and tabla_cropped.shape[-1] > 3:\n",
    "        tabla_cropped = tabla_cropped[:,:,:3]\n",
    "    result = ocr.ocr(tabla_cropped, cls=False)\n",
    "    \n",
    "    coords.append(t.tolist())\n",
    "    results.append(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc6e587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = {}\n",
    "tables_names = {}\n",
    "\n",
    "for c, r in zip(coords, results):\n",
    "    boxes = [line[0] for line in r]\n",
    "    txts = [line[1][0].strip() for line in r]\n",
    "    scores = [line[1][1] for line in r]\n",
    "    table, dict_attributes = clean_texts(txts)\n",
    "    all_tables[table] = dict_attributes\n",
    "    tables_names[table] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a53bcd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_to_names(pairs, tables_names):\n",
    "    new_pairs = []\n",
    "    tables_names = {str(v):k for k,v in tables_names.items()}\n",
    "    \n",
    "    for pair in pairs:\n",
    "        tabla_a, tabla_b = pair\n",
    "        tabla_a, tabla_b = tables_names[str(tabla_a)], tables_names[str(tabla_b)]\n",
    "        new_pairs.append((tabla_a, tabla_b))\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06cf2c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = pairs_to_names(pairs, tables_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07a2629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_keys(table_attributes):\n",
    "    return [attr for attr in table_attributes if \"_id\" in attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "20c2f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_keys(candidates, fks):\n",
    "    return list(set(candidates) - set(fks))\n",
    "\n",
    "def get_foreign_keys(table, candidates, pairs, lemmatizer=nlp_english):\n",
    "    def is_foreign_key(table_pair, candidates, table, lemmatizer):\n",
    "        pair = table_pair + \"_id\"\n",
    "        \n",
    "        table_pair_lemmatized = lemmatizer(table_pair)[0].lemma_\n",
    "        pair_lemmatized = table_pair_lemmatized + \"_id\"\n",
    "        \n",
    "        table_match = (table_pair == table or table_pair_lemmatized == table)\n",
    "        \n",
    "        pair_match = (pair in candidates and not table_match)\n",
    "        pair_lemmatized_match = (pair_lemmatized in candidates and not table_match)\n",
    "                \n",
    "        if pair_match:\n",
    "            return True, pair\n",
    "        elif pair_lemmatized_match:\n",
    "            return True, pair_lemmatized\n",
    "        else:\n",
    "            return False, \"\"\n",
    "    \"\"\"\n",
    "    Ejemplo:\n",
    "    table -> poems\n",
    "    candidates -> ['poems_id', 'users_id', 'categories_id']\n",
    "    pairs -> [('tokens', 'users'), ('poems', 'users'), ('poems', 'categories')]\n",
    "    \"\"\"\n",
    "    fks = {}\n",
    "    for pair in pairs:\n",
    "        if table not in pair:\n",
    "            continue\n",
    "        is_fk_pair0, p0 = is_foreign_key(table_pair=pair[0], candidates=candidates, \\\n",
    "                                         table=table, lemmatizer=lemmatizer)\n",
    "        is_fk_pair1, p1 = is_foreign_key(table_pair=pair[1], candidates=candidates, \\\n",
    "                                         table=table, lemmatizer=lemmatizer)\n",
    "        if is_fk_pair0:\n",
    "            fks[p0] = pair[0]\n",
    "        elif is_fk_pair1:\n",
    "            fks[p1] = pair[1]\n",
    "    return fks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "999d16d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: users\n",
      "Candidates: ['user_id']\n",
      "FKS: {}\n",
      "PKS: {'user_id': 'users'}\n",
      "####################\n",
      "Table: poems\n",
      "Candidates: ['poem_id', 'category_id', 'user_id']\n",
      "FKS: {'user_id': 'users', 'category_id': 'categories'}\n",
      "PKS: {'poem_id': 'poems'}\n",
      "####################\n",
      "Table: tokens\n",
      "Candidates: ['token_id', 'user_id']\n",
      "FKS: {'user_id': 'users'}\n",
      "PKS: {'token_id': 'tokens'}\n",
      "####################\n",
      "Table: categories\n",
      "Candidates: ['category_id']\n",
      "FKS: {}\n",
      "PKS: {'category_id': 'categories'}\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "## TODO: En base a esto construir las FK y las PK. Una vez hecho eso, tambi√©n definir las many to many.\n",
    "for k in all_tables.keys():  \n",
    "    candidates = extract_candidate_keys(all_tables[k].keys())\n",
    "    fks = get_foreign_keys(table=k, candidates=candidates, pairs=pairs)\n",
    "    pks = {pk: k for pk in get_primary_keys(candidates, fks.keys())}\n",
    "    print(f\"Table: {k}\")\n",
    "    print(f\"Candidates: {candidates}\")\n",
    "    print(f\"FKS: {fks}\")\n",
    "    print(f\"PKS: {pks}\")\n",
    "    print(\"####################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "730b7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pks_code(pks):\n",
    "    keys = pks.keys()\n",
    "    keys = \", \".join(keys)\n",
    "    return f\"PRIMARY KEY ({keys})\"\n",
    "\n",
    "def generate_fks_code(fks):\n",
    "    code = \"\"\n",
    "    for fk, table_reference in fks.items():\n",
    "        code += f\"FOREIGN KEY ({fk}) REFERENCES {table_reference}({fk}),\\n   \"\n",
    "    return code[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1b52bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_code(table, dict_attributes, primary_keys, foreign_keys):\n",
    "    '''\n",
    "    Crea una tabla de MySQL\n",
    "    '''\n",
    "    attributes_code = \"  \"\n",
    "    i = 0\n",
    "    for k, v in dict_attributes.items():\n",
    "        attributes_code += k + \" \" + v           \n",
    "        attributes_code += \",\\n   \"\n",
    "        i += 1\n",
    "    pks_code = generate_pks_code(primary_keys)\n",
    "    fks_code = generate_fks_code(foreign_keys)\n",
    "    if pks_code:\n",
    "        attributes_code += pks_code\n",
    "        attributes_code += \",\\n   \" if fks_code else \"\"\n",
    "    if fks_code:\n",
    "        attributes_code += fks_code\n",
    "    code = f\" CREATE TABLE {table} ( \\n {attributes_code} \\n );\"\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4c6aaef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CREATE TABLE users ( \n",
      "   user_id INT(11),\n",
      "   first_name VARCHAR(50),\n",
      "   last_name VARCHAR(50),\n",
      "   email VARCHAR(100),\n",
      "   username VARCHAR(30),\n",
      "   pass_phrase VARCHAR(500),\n",
      "   is_admin TINYINT(4),\n",
      "   date_registered DATETIME,\n",
      "   profile_pic VARCHAR(30),\n",
      "   registration_confirmed TINYINT(4),\n",
      "   PRIMARY KEY (user_id) \n",
      " )\n",
      " CREATE TABLE poems ( \n",
      "   poem_id INT(11),\n",
      "   title VARCHAR(200),\n",
      "   poem TEXT,\n",
      "   date_submitted DATETIME,\n",
      "   category_id INT(11),\n",
      "   user_id INT(11),\n",
      "   date_approved DATETIME,\n",
      "   PRIMARY KEY (poem_id),\n",
      "   FOREIGN KEY (user_id) REFERENCES users(user_id),\n",
      "   FOREIGN KEY (category_id) REFERENCES categories(category_id) \n",
      " )\n",
      " CREATE TABLE tokens ( \n",
      "   token_id INT(11),\n",
      "   token CHAR(64),\n",
      "   user_id INT(11),\n",
      "   token_expires DATETIME,\n",
      "   PRIMARY KEY (token_id),\n",
      "   FOREIGN KEY (user_id) REFERENCES users(user_id) \n",
      " )\n",
      " CREATE TABLE categories ( \n",
      "   category_id INT(11),\n",
      "   category VARCHAR(100),\n",
      "   PRIMARY KEY (category_id) \n",
      " )\n"
     ]
    }
   ],
   "source": [
    "for k, dict_attributes in all_tables.items():\n",
    "    candidates = extract_candidate_keys(all_tables[k].keys())\n",
    "    fks = get_foreign_keys(table=k, candidates=candidates, pairs=pairs)\n",
    "    pks = {pk: k for pk in get_primary_keys(candidates, fks.keys())}\n",
    "    print(create_code(k, dict_attributes, pks, fks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d84a8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_english = spacy.load(\"en_core_web_sm\")\n",
    "nlp_spanish = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ae5faca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['users_car']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.lemma_ for word in nlp_english(\"users_cars\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0bd437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ventanal']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.lemma_ for word in nlp_spanish(\"ventanales\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0465e5e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b8d053f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf164be",
   "metadata": {},
   "source": [
    "## Una vez que est√° todo hecho, unificar ac√° y detectar puntos de mejora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da2622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff655c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
