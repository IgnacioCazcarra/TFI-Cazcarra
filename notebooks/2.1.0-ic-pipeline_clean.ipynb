{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c25ec9",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "- Falta mover el código de line detection a prod.\n",
    "- Falta mover el código de OCR a prod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.detection.prediction_utils import choose_model, filter_predictions, visualize_boxes\n",
    "from src.slides_utils.slides_utils import predict_tiles\n",
    "from src.line_detection.hough import get_pairs, apply_hough, lines_to_points, hough_detecting, nearest_tabla_from_cardinalidad, clean_cardinalidades, reverse_dict, bbox_sum_n, sep_line, any_points_inside, point_inside\n",
    "from src.ocr_utils.ocr import get_ocr_model, get_lemmatizer, predict_ocr, generate_db, extract_candidate_keys, \\\n",
    "pairs_to_names, initial_guess_primary_keys, get_unchosen, get_foreign_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be94b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898df5b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# img_path = '/home/nacho/TFI-Cazcarra/data/imagenes_diagramas/ERDiagramsMySQL-9.png'\n",
    "img_path = '/home/nacho/TFI-Cazcarra/data/images_testing/test1.png'\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "min_size = 600\n",
    "max_size = 1333\n",
    "\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "img_tensor = transform(img)\n",
    "img.resize((int(s*0.75) for s in img.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bdef75",
   "metadata": {},
   "source": [
    "## Predicciones sobre los objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tablas = choose_model(model_name=\"retinanet\", object_to_predict=\"tablas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea76d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cardinalidades = choose_model(model_name=\"retinanet\", object_to_predict=\"cardinalidades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc19915",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablas_pred = model_tablas([img_tensor])[1][0]\n",
    "tablas_boxes, tablas_scores = filter_predictions(tablas_pred, nms_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f67509",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinalidades_pred = predict_tiles(img, model=model_cardinalidades, is_yolo=False, transform=transform)\n",
    "cardinalidades_boxes, cardinalidades_scores = filter_predictions(cardinalidades_pred, nms_threshold=0.5, \n",
    "                                                                 score_threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_boxes(img, cardinalidades_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4add9a",
   "metadata": {},
   "source": [
    "## Predicciones sobre las líneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_cardinalidades(img, lines, cardinalidades, plot=False):\n",
    "    dict_cardinalidades = {}\n",
    "    dict_lines = {f\"line_{i}\":l for i,l in enumerate(lines) if len(l)>1}\n",
    "    matches = 0\n",
    "    for c in cardinalidades:\n",
    "        augment = 0\n",
    "        offset = 1\n",
    "        flag = False\n",
    "        while not flag:\n",
    "            c_offset = bbox_sum_n(c, offset*augment).tolist()\n",
    "            for k, l in dict_lines.items():\n",
    "                start = l[0]\n",
    "                end = l[-1]\n",
    "                if point_inside(c_offset, start) or point_inside(c_offset, end):\n",
    "                    # Table + augment\n",
    "                    match_key= (c.tolist(), augment)\n",
    "                    dict_cardinalidades[str(match_key)] = k\n",
    "                    matches +=1\n",
    "                    flag = True\n",
    "                    break\n",
    "            if str(c.tolist()) not in dict_cardinalidades.keys():\n",
    "                augment += 2\n",
    "                print(f\"Increasing offset to {augment}\")\n",
    "    if plot:\n",
    "        display(plot_results(img, dict_cardinalidades, dict_lines))\n",
    "    new_dict_cardinalidades = reverse_dict(dict_cardinalidades)\n",
    "    return new_dict_cardinalidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35905206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lines(tablas, cardinalidades, img, offset_tablas=5, **kwargs):\n",
    "    offset = np.array([-offset_tablas, -offset_tablas, offset_tablas, offset_tablas]).reshape(1,4)\n",
    "    tablas = np.sum([tablas, offset])\n",
    "    img, all_lines = apply_hough(img, tablas, [])\n",
    "    all_points = lines_to_points(all_lines)\n",
    "    lines = hough_detecting(all_points)\n",
    "    cardinalidades = clean_cardinalidades(cardinalidades, tablas)\n",
    "    return unify_cardinalidades(img, lines, cardinalidades, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_line(line, tablas):\n",
    "    tabla_a = None\n",
    "    tabla_b = None\n",
    "    try:\n",
    "        cardinalidades = line.split(\"|\")\n",
    "        cardinalidades = [literal_eval(c) for c in cardinalidades]\n",
    "        cardinalidades_dist = {str(c[0]): c[1] for c in cardinalidades}\n",
    "        # TOP 2 con menos augment\n",
    "        cardinalidades = sorted(cardinalidades_dist, key=cardinalidades_dist.get)[:2]\n",
    "        tabla_a = nearest_tabla_from_cardinalidad(cardinalidades[0], tablas)\n",
    "        tabla_b = nearest_tabla_from_cardinalidad(cardinalidades[1], tablas)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al separar tablas! {e}. Chequear las bounding boxes pasadas. Salteando..\")\n",
    "    finally:\n",
    "        return (tabla_a, tabla_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbf686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(boxes_tablas, boxes_cardinalidades, img, **kwargs):\n",
    "    pairs = []\n",
    "    tablas = boxes_tablas.detach().numpy().astype(int)\n",
    "    cardinalidades = boxes_cardinalidades.detach().numpy().astype(int)\n",
    "        \n",
    "    for line_name, line in find_lines(img=img, tablas=tablas, cardinalidades=cardinalidades, **kwargs).items(): \n",
    "        tabla_a, tabla_b = sep_line(line, tablas)\n",
    "        if tabla_a and tabla_b:\n",
    "            pairs.append((tabla_a, tabla_b))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cad81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conexiones = get_pairs(tablas_boxes, cardinalidades_boxes, img=img, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "conexiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3294c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec1398",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr = get_ocr_model(det_algo=\"db\", rec_algo=\"svtr\", lang=\"en\")\n",
    "lemmatizer = get_lemmatizer(lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91915119",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablas_boxes_int = tablas_boxes.detach().numpy().astype(int)\n",
    "all_tables, tables_names = predict_ocr(img=img, tablas=tablas_boxes_int, ocr_model=ocr, scale_percent=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230980d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(generate_db(pairs=conexiones, all_tables=all_tables, tables_names=tables_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd2778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ece9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5227ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lemminflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lemminflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efe40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plural(word, lemmatizer):\n",
    "    return lemmatizer(word)[0]._.inflect('NNS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_many_to_many(table, table_candidates, tables_names, pairs):\n",
    "    # Con esto deberian quedar solo dos pero hay que preever que pasaría si hay más.\n",
    "    matches = [t for t in tables_names if ((t in table) or (get_plural(t, lemmatizer) in table)) and (t!=table)]\n",
    "    \n",
    "    i = 0\n",
    "    confirmed_matches = []\n",
    "    flag = False\n",
    "    while not flag and i<len(pairs):\n",
    "        pair = pairs[i]\n",
    "        if table not in pair:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "        if pair[0] in matches:\n",
    "            # Confirmamos que hay una tabla que aparece en el nombre de la m2m y\n",
    "            # tiene relación con ella.\n",
    "            confirmed_matches.append(pair[0]) \n",
    "        elif pair[1] in matches:\n",
    "            confirmed_matches.append(pair[1])\n",
    "            \n",
    "        if len(confirmed_matches) == 2:\n",
    "            # Si hay dos tablas que aparecen en el nombre de la m2m y tienen conexión con ella, es confirmado\n",
    "            flag = True\n",
    "        i+=1\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_valid_combs_fk(table, lemmatizer):\n",
    "    '''\n",
    "    Dado el nombre de una tabla, genera las combinaciones válidas. Osea, tabla+id, tabla+_id, \n",
    "    tabla_lematizada+id y tabla_lematizada+_id. También aplica a casos donde la tabla está en singular y \n",
    "    la PK en plural.\n",
    "    '''\n",
    "    table_lemmatized = lemmatizer(table)[0].lemma_\n",
    "    table_unlemmatized = get_plural(table, lemmatizer)\n",
    "    valid_combs = [table+\"id\", table+\"_id\", \n",
    "                   table_lemmatized+\"id\", table_lemmatized+\"_id\",\n",
    "                   table_unlemmatized+\"id\", table_unlemmatized+\"_id\"]\n",
    "    return list(set(valid_combs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_fk(table_pair, table_candidates, table_pair_candidates, lemmatizer):\n",
    "    '''\n",
    "    Match normal entre dos variantes con el nombre de la tabla.\n",
    "    '''\n",
    "    valid_combs = generate_valid_combs_fk(table_pair, lemmatizer)\n",
    "    possibilities = valid_combs\n",
    "    pair_possibilities = valid_combs + [\"id\"]\n",
    "    \n",
    "    for possibility in possibilities:\n",
    "        for pair_possibility in pair_possibilities:\n",
    "            if possibility in table_candidates and pair_possibility in table_pair_candidates:\n",
    "                table_candidates.remove(possibility) # Remuevo la fk de la lista de candidatos.\n",
    "                return (True, possibility, pair_possibility)\n",
    "    return (False, \"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684335ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_autofk(table, table_candidates, lemmatizer):\n",
    "    valid_combs = generate_valid_combs_fk(table, lemmatizer) + [\"id\"]\n",
    "    table_lemmatized = lemmatizer(table)[0].lemma_\n",
    "    table_unlemmatized = get_plural(table, lemmatizer)\n",
    "    fk = None\n",
    "    pk = None\n",
    "    \n",
    "    i = 0\n",
    "    while not fk and i<len(table_candidates):\n",
    "        t = table_candidates[i]\n",
    "        if ((table in t) or (table_lemmatized in t) or (table_unlemmatized in t)) and (t not in valid_combs):\n",
    "            fk = t\n",
    "        i+=1\n",
    "    j = 0\n",
    "    while not pk and j<len(valid_combs):\n",
    "        v = valid_combs[j]\n",
    "        if v in table_candidates:\n",
    "            pk = v\n",
    "        j+=1\n",
    "    \n",
    "    if pk:\n",
    "        if fk:\n",
    "            return (True, fk, pk)\n",
    "        else:\n",
    "            left_candidates = get_unchosen(table_candidates, valid_combs)\n",
    "            if len(left_candidates) == 1:\n",
    "                return (True, left_candidates[0], pk)\n",
    "    return (False, \"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_m2m(table, table_pair, table_candidates, table_pair_candidates, lemmatizer):\n",
    "    '''\n",
    "    Chequea si hay un atributo en común entre la tabla normal y la m2m. Si hay uno solo, se devuelve ese.\n",
    "    Si no, se sigue con la opción \"normal\" entre dos tablas convencionales (método 'match_fk').\n",
    "    '''\n",
    "    matches = [table_candidate for table_candidate in table_candidates if table_candidate in table_pair_candidates]\n",
    "    if len(matches) == 1:\n",
    "        # Si hubo match con solo un atributo.\n",
    "        # A las m2m no se les remueve la FK porque tambien es PK. \n",
    "        return (True, matches[0], matches[0])\n",
    "    else:\n",
    "        # Si no hubo un solo match, hacemos el chequeo \"normal\" con las combinaciones válidas de la tabla.\n",
    "        return match_fk(table_pair, table_candidates, table_pair_candidates, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_foreign_key(table, table_pair, table_candidates, table_pair_candidates, lemmatizer, m2m_tables,\n",
    "                   is_auto_fk=False):\n",
    "    '''\n",
    "    Se fija si hay un match entre un atributo con _id en su versión original y lematizada.\n",
    "    Soporta relaciones convencionales, relaciones many to many y auto foreign keys.\n",
    "    '''\n",
    "    if table_pair == table and not is_auto_fk:\n",
    "        return (False, \"\", \"\")\n",
    "\n",
    "    if is_auto_fk:\n",
    "        return match_autofk(table, table_candidates, lemmatizer)\n",
    "    elif table in m2m_tables:\n",
    "        # Si es una relación y la tabla es una \"many to many\"\n",
    "        return match_m2m(table, table_pair, table_candidates, table_pair_candidates, lemmatizer)\n",
    "    else:\n",
    "        # Si es una relación entre dos tablas \"estándar\".\n",
    "        return match_fk(table_pair, table_candidates, table_pair_candidates, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreign_keys(table, all_candidates, pairs, m2m_tables, lemmatizer=None, check_auto_fks=False):\n",
    "    \"\"\"\n",
    "    Ejemplo:\n",
    "    table -> poems\n",
    "    candidates -> ['poems_id', 'users_id', 'categories_id']\n",
    "    pairs -> [('tokens', 'users'), ('poems', 'users'), ('poems', 'categories')]\n",
    "    \"\"\"\n",
    "    if not lemmatizer:\n",
    "        lemmatizer = get_lemmatizer()\n",
    "    fks = {}\n",
    "    completed_pairs = []\n",
    "        \n",
    "    table_candidates = all_candidates[table]\n",
    "    for pair in pairs:\n",
    "        if table not in pair:\n",
    "            continue\n",
    "            \n",
    "        is_auto_fk = False\n",
    "        if pair[0] == pair[1] and check_auto_fks:\n",
    "            is_auto_fk = True\n",
    "        is_fk_pair0, table_att0, pair_att0 = is_foreign_key(table=table, table_pair=pair[0], \n",
    "                                                            table_candidates=table_candidates,\n",
    "                                                            table_pair_candidates=all_candidates[pair[0]], \n",
    "                                                            lemmatizer=lemmatizer, is_auto_fk=is_auto_fk,\n",
    "                                                            m2m_tables=m2m_tables)\n",
    "        is_fk_pair1, table_att1, pair_att1 = is_foreign_key(table=table, table_pair=pair[1], \n",
    "                                                            table_candidates=table_candidates,\n",
    "                                                            table_pair_candidates=all_candidates[pair[1]], \n",
    "                                                            lemmatizer=lemmatizer, is_auto_fk=is_auto_fk,\n",
    "                                                            m2m_tables=m2m_tables)\n",
    "        if is_fk_pair0:\n",
    "            fks[(table_att0, pair_att0)] = pair[0]\n",
    "            completed_pairs.append(pair)\n",
    "        elif is_fk_pair1:\n",
    "            fks[(table_att1, pair_att1)] = pair[1]\n",
    "            completed_pairs.append(pair)\n",
    "    return fks, completed_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f0102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = pairs_to_names(conexiones, tables_names)\n",
    "# Pasada inicial para extraer todos los candidatos.\n",
    "all_candidates = {}\n",
    "m2m_tables = []\n",
    "for k, dict_attributes in all_tables.items():\n",
    "    candidates = extract_candidate_keys(dict_attributes.keys(), [t for t in tables_names if t != k], lang=\"en\")\n",
    "    all_candidates[k] = candidates\n",
    "    if is_many_to_many(k, candidates, tables_names, pairs):\n",
    "        m2m_tables.append(k)\n",
    "\n",
    "all_tables_pks = {}\n",
    "all_tables_fks = {}\n",
    "\n",
    "# En esta segunda pasada se resuelven todas las relaciones menos la de auto fks.\n",
    "for k in all_tables.keys():    \n",
    "    pks = {pk: k for pk in initial_guess_primary_keys(k, all_candidates[k], get_lemmatizer(lang=\"en\"))}\n",
    "    fks, completed_pairs = get_foreign_keys(table=k, all_candidates=all_candidates, pairs=pairs,\\\n",
    "                                            m2m_tables=m2m_tables, check_auto_fks=False)\n",
    "    pairs = get_unchosen(pairs, completed_pairs)\n",
    "    pks = {**pks, **{pk: k for pk in get_unchosen(all_candidates[k], fks.keys())}}\n",
    "    all_tables_pks[k] = pks\n",
    "    all_tables_fks[k] = fks\n",
    "    \n",
    "# En esta tercera pasada se completan los auto-fks.\n",
    "for k in all_tables.keys():\n",
    "    fks, completed_pairs = get_foreign_keys(table=k, all_candidates=all_candidates, pairs=pairs,\\\n",
    "                                            m2m_tables=m2m_tables, check_auto_fks=True)\n",
    "    pairs = get_unchosen(pairs, completed_pairs)\n",
    "    if fks:\n",
    "        all_tables_fks[k] = {**all_tables_fks[k], **fks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pairs:\n",
    "    print(f\"Relationship between {p} could not be established.\") \n",
    "    print(\"Please check that the attributes are in the correct format.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pks_code(pks):\n",
    "    keys = pks.keys()\n",
    "    keys = \", \".join(keys)\n",
    "    return f\"PRIMARY KEY ({keys})\"\n",
    "\n",
    "\n",
    "def generate_fks_code(table, fks):\n",
    "    code = \"\"\n",
    "    for fk, table_reference in fks.items():\n",
    "        code += f\"ALTER TABLE {table} ADD FOREIGN KEY ({fk[0]}) REFERENCES {table_reference}({fk[1]}); \\n\"\n",
    "    return code\n",
    "\n",
    "\n",
    "def create_code(table, dict_attributes, primary_keys, foreign_keys):\n",
    "    '''\n",
    "    Crea una tabla de MySQL\n",
    "    '''\n",
    "    attributes_code = \"  \"\n",
    "    i = 0\n",
    "    for k, v in dict_attributes.items():\n",
    "        attributes_code += k + \" \" + v           \n",
    "        attributes_code += \",\\n   \"\n",
    "        i += 1\n",
    "    pks_code = generate_pks_code(primary_keys)\n",
    "    fks_code = generate_fks_code(table, foreign_keys)\n",
    "    if pks_code:\n",
    "        attributes_code += pks_code\n",
    "    code = f\" CREATE TABLE {table} ( \\n {attributes_code} \\n ); \\n\"\n",
    "    return code, fks_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_db(pairs, all_tables, tables_names, lang):\n",
    "    pairs = pairs_to_names(pairs, tables_names)\n",
    "    \n",
    "    all_candidates = {}\n",
    "    m2m_tables = []\n",
    "    # Primera pasada: Extraemos los candidatos y vemos qué tabla es m2m.\n",
    "    for k, dict_attributes in all_tables.items():\n",
    "        candidates = extract_candidate_keys(dict_attributes.keys(), [t for t in tables_names if t != k], lang=\"en\")\n",
    "        all_candidates[k] = candidates\n",
    "        if is_many_to_many(k, candidates, tables_names, pairs):\n",
    "            m2m_tables.append(k)\n",
    "    \n",
    "    all_tables_pks = {}\n",
    "    all_tables_fks = {}\n",
    "    # Segunda pasada: Se resuelven todas las relaciones menos la de auto fks.\n",
    "    for k in all_tables.keys():    \n",
    "        pks = {pk: k for pk in initial_guess_primary_keys(k, all_candidates[k], get_lemmatizer(lang=\"en\"))}\n",
    "        fks, completed_pairs = get_foreign_keys(table=k, all_candidates=all_candidates, pairs=pairs,\\\n",
    "                                                m2m_tables=m2m_tables, check_auto_fks=False)\n",
    "        pairs = get_unchosen(pairs, completed_pairs)\n",
    "        pks = {**pks, **{pk: k for pk in get_unchosen(all_candidates[k], fks.keys())}}\n",
    "        all_tables_pks[k] = pks\n",
    "        all_tables_fks[k] = fks\n",
    "        \n",
    "    all_code = \"\"\n",
    "    all_fks_code = \"\"\n",
    "    # Tercera pasada: Se completan los auto-fks y se genera el código.\n",
    "    for k, dict_attributes in all_tables.items():\n",
    "        fks, completed_pairs = get_foreign_keys(table=k, all_candidates=all_candidates, pairs=pairs,\\\n",
    "                                                m2m_tables=m2m_tables, check_auto_fks=True)\n",
    "        pairs = get_unchosen(pairs, completed_pairs)\n",
    "        if fks:\n",
    "            all_tables_fks[k] = {**all_tables_fks[k], **fks}\n",
    "            \n",
    "        code, fk_code = create_code(k, dict_attributes, \\\n",
    "                                    primary_keys=all_tables_pks[k], \\\n",
    "                                    foreign_keys=all_tables_fks[k])\n",
    "        all_code += code\n",
    "        all_fks_code += fk_code\n",
    "    return all_code + \"\\n\" + all_fks_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17798144",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_db(conexiones, all_tables, tables_names, lang=\"en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
